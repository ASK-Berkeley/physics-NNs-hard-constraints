import tensorflow as tf
import typing
from os.path import exists, join
from os import listdir, remove
from tqdm import tqdm
from shutil import copyfile
import h5py as h5
import tensorflow as tf

from ..constants import DatasetSplit

TF_RECORD_EXTENSION = '.tfrecords'


def int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def bytes_features(array):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(tf.cast(array, tf.float64)).numpy()]))


def save_tf_record(tf_example, record_file: str):
    assert not exists(record_file), \
        'Trying to write a tf_record to a file that already exists!'
    with tf.io.TFRecordWriter(record_file) as writer:
        writer.write(tf_example.SerializeToString())


def generate_tf_record(dataset: typing.Dict, collapse_dataset: bool = False):
    # Returns a tf_record of the dataset
    keys = dataset.keys()
    if collapse_dataset:
        keys = []

        def _visit(x):
            if isinstance(dataset[x], h5._hl.dataset.Dataset):
                keys.append(x)
        dataset.visit(_visit)
    serialized_obj = {
        key: bytes_features(dataset[key]) for key in keys
    }
    return tf.train.Example(features=tf.train.Features(feature=serialized_obj))


def dataset_to_tf_record(dataset: typing.Dict[str, typing.Any], out_file: str, collapse_dataset: bool = False):
    # Generates a tf_record containing the dataset as a single example.
    # Saves the tf_record to out_file
    save_tf_record(generate_tf_record(
        dataset, collapse_dataset=collapse_dataset), out_file)


def remove_tfrecords(directory: str):
    # Removes all tf_records from directory
    files = listdir(directory)
    files = [join(directory, f) for f in files
             if f[-1 * len(TF_RECORD_EXTENSION):] == TF_RECORD_EXTENSION]
    for f in files:
        remove(f)


def generate_splits(directory: str, split: typing.List[float] = [0.8, 0.1, 0.1]):
    # Generates splits of the dataset in the directory
    # Splits are generated by the split parameter [train, valid, test]
    # Splits are stored in the directory
    assert sum(split) == 1, 'Split does not sum to 1!'
    files = listdir(join(directory, str(DatasetSplit.All)))
    files = [join(directory, str(DatasetSplit.All), f) for f in files
             if f[-1 * len(TF_RECORD_EXTENSION):] == TF_RECORD_EXTENSION]
    num_files = len(files)
    split = [int(num_files * s) for s in split]

    # Ensure that we get all files
    split[0] += max(num_files - sum(split), 0)

    for i, f in enumerate(tqdm(files)):
        if i < split[0]:
            copyfile(f, f.replace(str(DatasetSplit.All), str(DatasetSplit.Train)))
        elif i < split[0] + split[1]:
            copyfile(f, f.replace(str(DatasetSplit.All),
                     str(DatasetSplit.Validation)))
        else:
            copyfile(f, f.replace(str(DatasetSplit.All), str(DatasetSplit.Test)))
